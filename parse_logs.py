import argparse
import csv
from pathlib import Path
import re
import ast

def parse_log_file(log_file_path: Path):
    """
    Parses a single training.log file to extract metrics.
    It takes the last found values for each metric within the last training run.
    """
    last_epoch_stats = {}
    testing_stats = {}
    epochs_trained = None

    try:
        content = log_file_path.read_text()

        # Split the log file by '**** Start Training ****' to handle multiple runs
        training_runs = content.split('**** Start Training ****')
        if len(training_runs) < 2:
            # If there's no 'Start Training' marker, there's no complete run to parse.
            return None

        # Process the last training run in the file
        last_run_content = training_runs[-1]

        # Check for early stopping first
        early_stop_match = re.search(r'Training early stops at epoch: (\d+)', last_run_content)
        if early_stop_match:
            try:
                epochs_trained = int(early_stop_match.group(1))
            except (ValueError, IndexError):
                pass  # Should not happen with the regex but good to be safe

        # Find the last epoch stats
        epoch_stats_matches = re.findall(r"Epoch stats: (\{.*\})", last_run_content)
        if epoch_stats_matches:
            try:
                # The last match is what we need
                stats_dict = ast.literal_eval(epoch_stats_matches[-1])
                last_epoch_stats['loss'] = stats_dict.get('loss')
                last_epoch_stats['val_loss'] = stats_dict.get('val_loss')
                last_epoch_stats['val_acc'] = stats_dict.get('val_acc')
                last_epoch_stats['val_auc'] = stats_dict.get('val_auc')
                
                # If early stopping wasn't found, get epoch from here
                if epochs_trained is None:
                    epochs_trained = stats_dict.get('epoch')

            except (SyntaxError, ValueError):
                pass  # Ignore malformed dictionaries

        # Find testing stats
        testing_stats_match = re.search(r'Testing Acc: ([\d.eE-]+), Testing Auc: ([\d.eE-]+)', last_run_content)
        if testing_stats_match:
            try:
                testing_stats['testing_acc'] = float(testing_stats_match.group(1))
                testing_stats['testing_auc'] = float(testing_stats_match.group(2))
            except ValueError:
                pass # Ignore conversion errors

        if epochs_trained is not None:
            last_epoch_stats['epochs_trained'] = epochs_trained

        if not last_epoch_stats and not testing_stats:
            return None

        return {**last_epoch_stats, **testing_stats}

    except Exception as e:
        print(f"Error reading or processing file {log_file_path}: {e}")
        return None

def main(root_dir: Path, output_file: Path):
    """
    Finds all training.log files, parses them, and writes results to a CSV.
    """
    all_log_files = root_dir.rglob('training.log')
    log_files = [f for f in all_log_files if 'inner' not in str(f)]
    print(f"Found {len(log_files)} log files (excluding 'inner' paths) in {root_dir}")

    if not log_files:
        print(f"No 'training.log' files found (excluding 'inner' paths) in '{root_dir}' and its subdirectories.")
        return

    results = []
    for log_file in log_files:
        data = parse_log_file(log_file)
        if data:
            experiment_name = log_file.parent.parent.name
            fold = log_file.parent.name
            result_row = {'experiment': experiment_name, 'fold': fold, **data}
            results.append(result_row)
            print(f"Parsed {log_file}")

    if not results:
        print("No data could be extracted from any log file.")
        return

    # Determine all possible fieldnames from the results to handle missing values gracefully
    all_keys = set()
    for r in results:
        all_keys.update(r.keys())

    # Define the desired order of columns
    fieldnames_order = ['experiment', 'fold', 'epochs_trained', 'testing_auc', 'testing_acc', 'val_auc', 'val_acc', 'val_loss', 'loss']
    # Filter and order the found keys
    sorted_fieldnames = [f for f in fieldnames_order if f in all_keys]
    # Add any other keys that might have been found but are not in the predefined order
    sorted_fieldnames.extend(sorted([k for k in all_keys if k not in sorted_fieldnames]))


    with open(output_file, 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=sorted_fieldnames, extrasaction='ignore')
        writer.writeheader()
        writer.writerows(results)

    print(f"Results successfully written to {output_file}")
    print(f"Processed {len(log_files)} log files.")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Parse training logs and aggregate results into a CSV file.")
    parser.add_argument(
        '--root-directory', 
        type=Path, 
        required=True, 
        help="The root directory to search for training.log files (e.g., 'exp')."
    )
    parser.add_argument(
        '--output-file', 
        type=Path, 
        default=Path('training_results.csv'), 
        help="The path to the output CSV file."
    )
    args = parser.parse_args()

    main(args.root_directory, args.output_file) 