{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86cc74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcsparser import parse \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef297bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = r\"Y:/g/data/eu59/data_flowmop/fig_4_data/ANUDC_16\"\n",
    "raw_cells_location = root + \"/FCS_files/cells_panel/\"\n",
    "raw_plasma_location = root + \"/FCS_files/plasma_panel/\"\n",
    "\n",
    "# Usage example:\n",
    "csv_path = root + \"/anu_dc_metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047b2632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from typing import List\n",
    "import logging\n",
    "\n",
    "def move_fcs_files_to_main_dir(main_dir: str) -> None:\n",
    "    \"\"\"Move all .fcs files from subdirectories into the main directory and delete empty subdirectories.\n",
    "\n",
    "    Args:\n",
    "        main_dir (str): The path to the main directory containing subdirectories with .fcs files.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the main directory does not exist.\n",
    "        Exception: For any unexpected errors during file operations.\n",
    "\n",
    "    Example:\n",
    "        move_fcs_files_to_main_dir(r\"Y:/g/data/eu59/data_flowmop/fig_4_data/ANUDC_16/FCS_files/cells_panel/\")\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    if not os.path.isdir(main_dir):\n",
    "        raise FileNotFoundError(f\"Main directory '{main_dir}' does not exist.\")\n",
    "\n",
    "    # Walk through all subdirectories\n",
    "    for root_dir, dirs, files in os.walk(main_dir, topdown=False):\n",
    "        if root_dir == main_dir:\n",
    "            continue  # Skip the main directory itself\n",
    "\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.fcs'):\n",
    "                src_path = os.path.join(root_dir, file)\n",
    "                dest_path = os.path.join(main_dir, file)\n",
    "                # If file with same name exists, rename to avoid overwrite\n",
    "                if os.path.exists(dest_path):\n",
    "                    base, ext = os.path.splitext(file)\n",
    "                    i = 1\n",
    "                    while os.path.exists(os.path.join(main_dir, f\"{base}_{i}{ext}\")):\n",
    "                        i += 1\n",
    "                    dest_path = os.path.join(main_dir, f\"{base}_{i}{ext}\")\n",
    "                    logger.warning(f\"File {file} exists in main directory. Renaming to {os.path.basename(dest_path)}.\")\n",
    "                try:\n",
    "                    shutil.move(src_path, dest_path)\n",
    "                    logger.info(f\"Moved {src_path} to {dest_path}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to move {src_path} to {dest_path}: {e}\")\n",
    "\n",
    "        # After moving files, remove the subdirectory if empty\n",
    "        try:\n",
    "            if not os.listdir(root_dir):\n",
    "                os.rmdir(root_dir)\n",
    "                logger.info(f\"Removed empty directory: {root_dir}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to remove directory {root_dir}: {e}\")\n",
    "\n",
    "# Usage example:\n",
    "move_fcs_files_to_main_dir(raw_plasma_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f17b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import os\n",
    "import logging\n",
    "\n",
    "def remove_spaces_from_filenames(directory: str) -> None:\n",
    "    \"\"\"\n",
    "    Recursively walks through the given directory and its subdirectories,\n",
    "    renaming all files to remove spaces from their filenames.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The root directory to start renaming files.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the provided directory does not exist.\n",
    "        Exception: For any unexpected errors during file operations.\n",
    "\n",
    "    Example:\n",
    "        remove_spaces_from_filenames(r\"Y:/g/data/eu59/data_flowmop/fig_4_data/ANUDC_16/FCS_files/plasma_panel/\")\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    if not os.path.isdir(directory):\n",
    "        raise FileNotFoundError(f\"Directory '{directory}' does not exist.\")\n",
    "\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            if \" \" in filename:\n",
    "                old_path = os.path.join(root, filename)\n",
    "                new_filename = filename.replace(\" \", \"\")\n",
    "                new_path = os.path.join(root, new_filename)\n",
    "                # Avoid overwriting existing files\n",
    "                if os.path.exists(new_path):\n",
    "                    base, ext = os.path.splitext(new_filename)\n",
    "                    i = 1\n",
    "                    while os.path.exists(os.path.join(root, f\"{base}_{i}{ext}\")):\n",
    "                        i += 1\n",
    "                    new_path = os.path.join(root, f\"{base}_{i}{ext}\")\n",
    "                    logger.warning(\n",
    "                        f\"File {new_filename} already exists. Renaming to {os.path.basename(new_path)}.\"\n",
    "                    )\n",
    "                try:\n",
    "                    os.rename(old_path, new_path)\n",
    "                    logger.info(f\"Renamed '{old_path}' to '{new_path}'\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to rename '{old_path}' to '{new_path}': {e}\")\n",
    "\n",
    "remove_spaces_from_filenames(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc0824ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PN names shared across all files: {'FSC-W', 'SSC-W', 'Qdot 605-A', 'FITC-A', 'FSC-H', 'Alexa Fluor 700-A', 'Time', 'SSC-H', 'APC-Cy7-A', 'SSC-A', 'PerCP-Cy5-5-A', 'PE-Cy7-A', 'Alexa Fluor 405-A', 'FSC-A', 'Alexa Fluor 430-A', 'PE-A', 'PE-Texas Red-A', 'APC-A'}\n",
      "PN names shared across all files: {'Alexa Fluor 488-A', 'Qdot 605-A', 'Alexa Fluor 700-A', 'Time', 'APC-Cy7-A', 'SSC-A', 'PE-Cy7-A', '7-AAD-A', 'FSC-A', 'Alexa Fluor 405-A', 'Alexa Fluor 430-A', 'PE-A', 'PE-Texas Red-A', 'APC-A'}\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "def extract_pn_metadata(metadata: Dict[str, Any]) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Extracts all values from the metadata dictionary where the key starts with '$P' and ends with 'N'.\n",
    "\n",
    "    Args:\n",
    "        metadata (Dict[str, Any]): The metadata dictionary parsed from an FCS file.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of values corresponding to keys that start with '$P' and end with 'N'.\n",
    "\n",
    "    Example:\n",
    "        >>> meta = parse(raw_cells_location + os.listdir(raw_cells_location)[0], meta_data_only=True)\n",
    "        >>> extract_pn_metadata(meta)\n",
    "        ['FSC-A', 'SSC-A', ...]\n",
    "    \"\"\"\n",
    "    # List to store the values of keys matching the pattern\n",
    "    pn_values: List[Any] = [\n",
    "        value\n",
    "        for key, value in metadata.items()\n",
    "        if key.startswith(\"$P\") and key.endswith(\"N\")\n",
    "    ]\n",
    "    return pn_values\n",
    "\n",
    "from typing import Set, Any\n",
    "\n",
    "def get_shared_pn_names(directory: str) -> Set[Any]:\n",
    "    \"\"\"\n",
    "    Extracts the set of PN names (parameter names) that are shared across all FCS files in the given directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing FCS files.\n",
    "\n",
    "    Returns:\n",
    "        Set[Any]: Set of PN names shared by all files.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the directory does not exist or contains no FCS files.\n",
    "        Exception: If parsing any file fails.\n",
    "    \n",
    "    Example:\n",
    "        >>> shared_pn_names = get_shared_pn_names(raw_cells_location)\n",
    "        >>> print(shared_pn_names)\n",
    "        {'FSC-A', 'SSC-A', ...}\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    # List all FCS files in the directory\n",
    "    files = [f for f in os.listdir(directory) if f.lower().endswith('.fcs')]\n",
    "\n",
    "    shared_pn_names: Set[Any] = set()\n",
    "    for idx, filename in enumerate(files):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        meta = parse(file_path, meta_data_only=True)\n",
    "        pn_names = set(extract_pn_metadata(meta))\n",
    "        if idx == 0:\n",
    "            shared_pn_names = pn_names\n",
    "        else:\n",
    "            shared_pn_names &= pn_names\n",
    "\n",
    "    return shared_pn_names\n",
    "\n",
    "# Usage example:\n",
    "cells_shared_pn_names = get_shared_pn_names(raw_cells_location)\n",
    "plasma_shared_pn_names = get_shared_pn_names(raw_plasma_location)\n",
    "print(f\"PN names shared across all files: {cells_shared_pn_names}\")\n",
    "print(f\"PN names shared across all files: {plasma_shared_pn_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e3a0af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "markers = pd.DataFrame(columns=list(cells_shared_pn_names))\n",
    "markers.drop(\"Time\", axis = 1).to_csv(root + \"/cells_marker_names.csv\", index=False)\n",
    "\n",
    "markers = pd.DataFrame( columns=list(plasma_shared_pn_names))\n",
    "markers.drop(\"Time\", axis = 1).to_csv(root + \"/plasma_marker_names.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a4eb499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding flowmop markers\n",
    "import pandas as pd\n",
    "\n",
    "markers = pd.read_csv(root + \"/cells_marker_names.csv\")\n",
    "markers = list(markers.columns )\n",
    "markers.extend([marker for marker in [\"passedlod\",\"passeddebris\",\"passedtime\",\"passeddoublet\",\"passedfinal\"]])\n",
    "pd.DataFrame(columns = markers).to_csv(root + \"/cells_marker_names_with_flowmop.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1b732fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files present in directory: 244\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_present_files_from_csv(\n",
    "    csv_path: str, directory: str, filename_column: str = \"filename\"\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Reads a CSV file and checks which files listed in the specified filename column\n",
    "    are present in the given directory.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file containing file metadata.\n",
    "        directory (str): Directory to check for file presence.\n",
    "        filename_column (str): Name of the column in the CSV that contains filenames.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of filenames that are present in the directory.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the CSV file or directory does not exist.\n",
    "        ValueError: If the filename column is not found in the CSV.\n",
    "\n",
    "    Example:\n",
    "        >>> present_files = get_present_files_from_csv(\n",
    "        ...     \"metadata.csv\", \"Y:/g/data/eu59/data_flowmop/fig_4_data/ANUDC_16/FCS_files/cells_panel/\"\n",
    "        ... )\n",
    "        >>> print(present_files)\n",
    "        ['file1.fcs', 'file2.fcs']\n",
    "    \"\"\"\n",
    "    # Check if CSV and directory exist\n",
    "    if not os.path.isfile(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "    if not os.path.isdir(directory):\n",
    "        raise FileNotFoundError(f\"Directory not found: {directory}\")\n",
    "\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(csv_path, sep=\";\")\n",
    "    if filename_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{filename_column}' not found in CSV.\")\n",
    "\n",
    "    # Normalize directory file list for comparison\n",
    "    dir_files = set(os.listdir(directory))\n",
    "\n",
    "    # Check which files from CSV are present in the directory\n",
    "    present_files = [\n",
    "        fname for fname in df[filename_column].astype(str) if fname in dir_files\n",
    "    ]\n",
    "\n",
    "    return present_files\n",
    "\n",
    "cells_present_files = get_present_files_from_csv(csv_path, raw_cells_location)\n",
    "plasma_present_files = get_present_files_from_csv(csv_path, raw_plasma_location)\n",
    "print(f\"Files present in directory: {len(cells_present_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ce50ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered metadata saved to: Y:/g/data/eu59/data_flowmop/fig_4_data/ANUDC_16/anu_dc_metadata_cellsfiltered.csv\n",
      "Filtered rows: 244\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Define the columns to keep\n",
    "COLUMNS_TO_KEEP: List[str] = [\n",
    "    \"filename\",\n",
    "    \"tumour_model\",\n",
    "    \"day\",\n",
    "    \"tumour\",\n",
    "    \"treatment\",\n",
    "    \"Age (weeks)\",\n",
    "    \"Spleen (mg) D8\",\n",
    "    \"Spleen (mg) D15\",\n",
    "    \"Right_tumour_(mg) D8\",\n",
    "    \"Right Tumour (mg) D15\",\n",
    "]\n",
    "\n",
    "def filter_and_save_metadata(\n",
    "    csv_path: str,\n",
    "    present_files: List[str],\n",
    "    columns_to_keep: List[str],\n",
    "    suffix: str = \"_filtered\"\n",
    ") -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Filter the metadata CSV to only include rows for files present in the directory,\n",
    "    save the filtered DataFrame with a suffix, and return the filtered CSV path and\n",
    "    the list of files present in the filtered metadata.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the original metadata CSV.\n",
    "        present_files (List[str]): List of filenames present in the directory.\n",
    "        columns_to_keep (List[str]): List of columns to retain in the filtered CSV.\n",
    "        suffix (str): Suffix to append to the filtered CSV filename.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, List[str]]: Path to the filtered CSV file and list of files present in the filtered metadata.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the CSV file does not exist.\n",
    "        ValueError: If required columns are missing.\n",
    "\n",
    "    Example:\n",
    "        >>> filtered_csv, filtered_files = filter_and_save_metadata(\n",
    "        ...     \"metadata.csv\", [\"file1.fcs\", \"file2.fcs\"], [\"filename\", \"treatment\"]\n",
    "        ... )\n",
    "        >>> print(filtered_csv)\n",
    "        'metadata_filtered.csv'\n",
    "        >>> print(filtered_files)\n",
    "        ['file1.fcs', 'file2.fcs']\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV file not found: {csv_path}\")\n",
    "\n",
    "    # Read the original CSV (assume same separator as before)\n",
    "    df = pd.read_csv(csv_path, sep=\";\")\n",
    "\n",
    "    # Check for required columns\n",
    "    missing_cols = [col for col in columns_to_keep if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in CSV: {missing_cols}\")\n",
    "\n",
    "    # Filter rows where 'filename' is in present_files\n",
    "    filtered_df = df[df[\"filename\"].astype(str).isin(present_files)][columns_to_keep]\n",
    "\n",
    "    # Only keep files that are present in the filtered metadata\n",
    "    filtered_files: List[str] = filtered_df[\"filename\"].astype(str).tolist()\n",
    "\n",
    "    # Construct output path\n",
    "    base, ext = os.path.splitext(csv_path)\n",
    "    filtered_csv_path = f\"{base}{suffix}{ext}\"\n",
    "\n",
    "    # Save filtered DataFrame\n",
    "    filtered_df.to_csv(filtered_csv_path, sep=\";\", index=False)\n",
    "    print(f\"Filtered metadata saved to: {filtered_csv_path}\")\n",
    "    print(f\"Filtered rows: {len(filtered_df)}\")\n",
    "\n",
    "    return filtered_csv_path, filtered_files\n",
    "\n",
    "# Usage example:\n",
    "filtered_cell_csv_path, filtered_cell_files = filter_and_save_metadata(\n",
    "    csv_path=csv_path,\n",
    "    present_files=cells_present_files,\n",
    "    columns_to_keep=COLUMNS_TO_KEEP,\n",
    "    suffix=\"_cellsfiltered\"\n",
    ")\n",
    "# Now, filtered_cell_files contains only the files present in the filtered metadata.\n",
    "# You can use this list to move/copy files to a separate directory as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e86834c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['200825_PIIOVo2_FC_4T1R_Bld_D7_C1_003.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C1_005.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C1eC100_002.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C1eC1000_004.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C1Nil_001.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C2_009.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C2eC100_006.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C2eC100_010.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C2eC1000_008.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C2Nil_007.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C3_011.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C3_015.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C3eC100_014.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C3eC1000_012.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C3Nil_013.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C4_017.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C4eC100_018.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C4eC1000_016.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C4eC1000_020.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C4Nil_019.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C5_021.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C5eC100_022.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C5eC1000_023.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C5eC1000_024.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C5Nil_025.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C6_028.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C6eC100_026.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C6eC100_030.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C6eC1000_027.fcs',\n",
       " '200825_PIIOVo2_FC_4T1R_Bld_D7_C6Nil_029.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C1_003.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C1_005.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C1eC100_002.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C1eC1000_004.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C1Nil_001.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C2_009.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C2eC100_006.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C2eC100_010.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C2eC1000_008.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C2Nil_007.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C3_011.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C3_015.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C3eC100_014.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C3eC1000_012.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C3Nil_013.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C4_017.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C4eC100_018.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C4eC1000_016.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C4eC1000_020.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C4Nil_019.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C5_021.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C5eC100_022.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C5eC1000_023.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C5eC1000_024.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C5Nil_025.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C6_028.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C6eC100_026.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C6eC100_030.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C6eC1000_027.fcs',\n",
       " '200901_PIIOVo2_FC_4T1R_Bld_D14_C6Nil_029.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C1_002.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C1_003.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C1_004.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C1_005.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C1Nil_001.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C2_006.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C2_008.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C2_009.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C2_010.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C2Nil_007.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C3_011.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C3_012.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C3_014.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C3_015.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C3Nil_013.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C4_016.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C4_017.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C4_018.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C4_020.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C4Nil_019.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C5_021.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C5_022.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C5_023.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C5_024.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C5Nil_025.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C6_026.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C6_027.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C6_028.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C6_030.fcs',\n",
       " '200915_MPTLVo4_FC_4T1R_Bld_D7_C6Nil_029.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C1_002.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C1_003.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C1_004.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C1_005.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C1Nil_001.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C2_006.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C2_008.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C2_009.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C2_010.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C2Nil_007.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C3_011.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C3_012.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C3_014.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C3_015.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C3Nil_013.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C4_016.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C4_017.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C4_018.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C4_020.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C4Nil_019.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C5_021.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C5_022.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C5_023.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C5_024.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C5Nil_025.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C6_027.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C6_028.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C6_029.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C6_030.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C6Nil_026.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C7_031.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C7_033.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C7_034.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C7_035.fcs',\n",
       " '201013_MPTLVo6_FC_CT26R_Bld_D7_C7Nil_032.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C1_002.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C1_003.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C1_004.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C1Nil_001.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C2_008.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C2_009.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C2_010.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C2Nil_007.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C3_012.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C3_014.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C3_015.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C3Nil_013.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C4_016.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C4_017.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C4_018.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C4_020.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C4Nil_019.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C5_021.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C5_022.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C5_024.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C5Nil_025.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C6_027.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C6_028.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C6_029.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C6_030.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C6Nil_026.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C7_031.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C7_035.fcs',\n",
       " '201020_MPTLVo6_FC_CT26R_Bld_D14_C7Nil_032.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C1_002.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C1_003.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C1_004.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C1_005.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C2_006.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C2_008.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C2_009.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C2_010.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C3_011.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C3_012.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C3_014.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C3_015.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C4_016.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C4_017.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C4_018.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C4_020.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C5_021.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C5_022.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C5_023.fcs',\n",
       " '201111_MPTLVo7_FC_CT26R_Bld_D7_C5_024.fcs',\n",
       " '201111_PIIOVo4_FC_4T1R_Bld_D7_C6_002.fcs',\n",
       " '201111_PIIOVo4_FC_4T1R_Bld_D7_C6_004.fcs',\n",
       " '201111_PIIOVo4_FC_4T1R_Bld_D7_C6eC10000_001.fcs',\n",
       " '201111_PIIOVo4_FC_4T1R_Bld_D7_C6eC10000_003.fcs',\n",
       " '201111_PIIOVo4_FC_4T1R_Bld_D7_C6eC10000_005.fcs',\n",
       " '201111_PIIOVo4_FC_4T1R_Bld_D7_C7_006.fcs',\n",
       " '201111_PIIOVo4_FC_4T1R_Bld_D7_C7_008.fcs',\n",
       " '201111_PIIOVo4_FC_4T1R_Bld_D7_C7_010.fcs',\n",
       " '201111_PIIOVo4_FC_4T1R_Bld_D7_C7eC10000_007.fcs',\n",
       " '201111_PIIOVo4_FC_4T1R_Bld_D7_C7eC10000_009.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C1_002.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C1_003.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C1_004.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C1_005.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C2_006.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C2_008.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C2_009.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C2_010.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C3_011.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C3_012.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C3_014.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C3_015.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C4_016.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C4_017.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C4_018.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C4_020.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C5_021.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C5_022.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C5_023.fcs',\n",
       " '201117_MPTLVo7_FC_CT26R_Bld_D14_C5_024.fcs',\n",
       " '201117_PIIOVo4_FC_4T1R_Bld_D14_C6_002.fcs',\n",
       " '201117_PIIOVo4_FC_4T1R_Bld_D14_C6_004.fcs',\n",
       " '201117_PIIOVo4_FC_4T1R_Bld_D14_C6eC10000_001.fcs',\n",
       " '201117_PIIOVo4_FC_4T1R_Bld_D14_C6eC10000_003.fcs',\n",
       " '201117_PIIOVo4_FC_4T1R_Bld_D14_C6eC10000_005.fcs',\n",
       " '201117_PIIOVo4_FC_4T1R_Bld_D14_C7_006.fcs',\n",
       " '201117_PIIOVo4_FC_4T1R_Bld_D14_C7_008.fcs',\n",
       " '201117_PIIOVo4_FC_4T1R_Bld_D14_C7_010.fcs',\n",
       " '201117_PIIOVo4_FC_4T1R_Bld_D14_C7eC10000_007.fcs',\n",
       " '201117_PIIOVo4_FC_4T1R_Bld_D14_C7eC10000_009.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C1_002.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C1_003.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C1_004.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C1_005.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C1Nil_001.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C2_006.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C2_008.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C2_009.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C2_010.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C2Nil_007.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C3_011.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C3_012.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C3_014.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C3_015.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C3Nil_013.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C4_016.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C4_017.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C4_018.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C4_020.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C4Nil_019.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C5_021.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C5_022.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C5_023.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C5_024.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C5Nil_025.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C6_026.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C6_027.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C6_028.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C6_030.fcs',\n",
       " '20222_MPTLVo4_FC_4T1R_Bld_D14_C6Nil_029.fcs']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from typing import List\n",
    "\n",
    "def copy_files_by_basename(\n",
    "    source_dir: str,\n",
    "    basenames: List[str],\n",
    "    dest_dir: str,\n",
    "    file_extension: str = \".fcs\"\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Copies files from the source directory to the destination directory if their base names match any in the provided list.\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): Path to the directory containing the source files.\n",
    "        basenames (List[str]): List of base filenames (without extension) to match.\n",
    "        dest_dir (str): Path to the directory where matched files will be copied.\n",
    "        file_extension (str): File extension to match (default: \".fcs\").\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of file paths that were successfully copied.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the source directory does not exist.\n",
    "        OSError: If a file cannot be copied.\n",
    "    \n",
    "    Example:\n",
    "        >>> copy_files_by_basename(\n",
    "        ...     source_dir=\"raw_data\",\n",
    "        ...     basenames=[\"200825_PIIOVo2_FC_4T1R_Bld_D7_C1_003\", \"20222_MPTLVo4_FC_4T1R_Bld_D14_C5_022\"],\n",
    "        ...     dest_dir=\"filtered_data\"\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(source_dir):\n",
    "        raise FileNotFoundError(f\"Source directory not found: {source_dir}\")\n",
    "\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    copied_files: List[str] = []\n",
    "\n",
    "    # Create a set for faster lookup\n",
    "    basename_set = set(basenames)\n",
    "\n",
    "    for fname in os.listdir(source_dir):\n",
    "        file_base, ext = os.path.splitext(fname)\n",
    "        if ext.lower() == file_extension.lower() and file_base in basename_set:\n",
    "            src_path = os.path.join(source_dir, fname)\n",
    "            dst_path = os.path.join(dest_dir, fname)\n",
    "            try:\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "                copied_files.append(dst_path)\n",
    "            except OSError as e:\n",
    "                print(f\"Error copying {src_path} to {dst_path}: {e}\")\n",
    "\n",
    "    print(f\"Copied {len(copied_files)} files to {dest_dir}\")\n",
    "    return copied_files\n",
    "\n",
    "# Usage example:\n",
    "# Suppose you have a list of base filenames (without extension) you want to copy:\n",
    "# filtered_basenames = [os.path.splitext(f)[0] for f in filtered_cell_files]\n",
    "# copied = copy_files_by_basename(\n",
    "#     source_dir=\"Y:/g/data/eu59/data_flowmop/fig_4_data/ANUDC_16/raw_fcs\",\n",
    "#     basenames=filtered_basenames,\n",
    "#     dest_dir=\"Y:/g/data/eu59/data_flowmop/fig_4_data/ANUDC_16/filtered_fcs\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f9562d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from typing import List, Optional, Tuple\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def parse_filename_for_metadata(\n",
    "    filename: str,\n",
    ") -> Optional[Tuple[str, str, str, int]]:\n",
    "    \"\"\"\n",
    "    Parses a filename to extract metadata components for matching.\n",
    "\n",
    "    The expected filename format is like:\n",
    "    '200707_MPTLVo2_MacMicLP_4T1R_Bld_D8_C1Nil_001.fcs'\n",
    "\n",
    "    Args:\n",
    "        filename (str): The filename to parse.\n",
    "\n",
    "    Returns:\n",
    "        Optional[Tuple[str, str, str, int]]: A tuple containing\n",
    "        (experiment, tumour_model, cage, sample) if parsing is successful,\n",
    "        otherwise None.\n",
    "    \"\"\"\n",
    "    name_without_ext = os.path.splitext(filename)[0]\n",
    "    parts = name_without_ext.split(\"_\")\n",
    "\n",
    "    try:\n",
    "        # date = parts[0]\n",
    "        experiment = parts[1]\n",
    "        tumour_model = parts[3]\n",
    "        cage_part = parts[6]\n",
    "        sample = int(parts[7])\n",
    "\n",
    "        # Extract cage from format like 'C1Nil' -> 'C1'\n",
    "        cage_match = re.match(r\"(C\\d+)\", cage_part)\n",
    "        if not cage_match:\n",
    "            return None\n",
    "        cage = cage_match.group(1)\n",
    "\n",
    "        return (experiment, tumour_model, cage, sample)\n",
    "        # return (date, experiment, tumour_model, cage, sample)\n",
    "    except (IndexError, ValueError):\n",
    "        # Handles cases where parts are missing or sample is not an integer\n",
    "        logging.warning(f\"Failed to parse filename: {filename}\")\n",
    "        return None\n",
    "\n",
    "def filter_and_save_metadata_plasma(\n",
    "    csv_path: str,\n",
    "    dir_files: List[str],\n",
    "    columns_to_keep: List[str],\n",
    "    suffix: str = \"_filtered\",\n",
    ") -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Filter the metadata CSV to include only rows that match files present\n",
    "    in the directory, based on a complex matching logic, and save the\n",
    "    filtered DataFrame with a specified suffix.\n",
    "\n",
    "    Matching is based on experiment, tumour model, cage, and sample\n",
    "    number parsed from the filename.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the original metadata CSV.\n",
    "        dir_files (List[str]): List of filenames present in the directory.\n",
    "        columns_to_keep (List[str]): List of columns to retain in the filtered CSV.\n",
    "        suffix (str): Suffix to add to the filtered CSV filename.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, List[str]]: A tuple containing the path to the filtered\n",
    "        CSV file and a list of filenames that are present in both the\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the CSV file does not exist.\n",
    "        ValueError: If required columns are missing from the CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(csv_path, sep=\";\")\n",
    "\n",
    "    # --- New matching logic ---\n",
    "\n",
    "    # NOTE: The following column names are assumed to exist in the metadata CSV:\n",
    "    # 'experiment', 'tumour model', 'cage', 'sample'.\n",
    "    # If the 'cage' column has a different name, it needs to be updated below.\n",
    "    matching_cols = [\"experiment\", \"tumour_model\", \"cage\", \"sample\"]\n",
    "\n",
    "    # 1. Parse all present filenames and create a mapping from parsed keys to original filenames\n",
    "    present_files_map = {}\n",
    "    for file in dir_files:\n",
    "        key = parse_filename_for_metadata(file)\n",
    "        if key:\n",
    "            if key not in present_files_map:\n",
    "                present_files_map[key] = []\n",
    "            present_files_map[key].append(file)\n",
    "    present_file_keys = set(present_files_map.keys())\n",
    "\n",
    "    # 2. Create a unique identifier tuple for each row in the DataFrame.\n",
    "    #    Ensure data types are consistent for comparison.\n",
    "    df_match_subset = df[matching_cols].copy()\n",
    "    df_match_subset[\"experiment\"] = df_match_subset[\"experiment\"].astype(str)\n",
    "    df_match_subset[\"tumour_model\"] = df_match_subset[\"tumour_model\"].astype(str)\n",
    "    df_match_subset[\"cage\"] = df_match_subset[\"cage\"].astype(str)\n",
    "    df_match_subset[\"sample\"] = df_match_subset[\"sample\"].astype(int)\n",
    "\n",
    "    df_keys = [tuple(x) for x in df_match_subset.to_numpy()]\n",
    "\n",
    "    # 3. Create a boolean mask for rows that have a key present in our set\n",
    "    mask = [key in present_file_keys for key in df_keys]\n",
    "\n",
    "    # 4. Filter the DataFrame using the mask and select desired columns\n",
    "    filtered_df = df[mask][columns_to_keep]\n",
    "\n",
    "    # --- Find matching files ---\n",
    "    # Get the unique keys that are present in the filtered dataframe\n",
    "    filtered_keys = {key for i, key in enumerate(df_keys) if mask[i]}\n",
    "\n",
    "    # Use these keys to find the corresponding filenames\n",
    "    matching_files = []\n",
    "    for key in sorted(list(filtered_keys)):\n",
    "        if key in present_files_map:\n",
    "            matching_files.extend(present_files_map[key])\n",
    "\n",
    "    print(\"matching files len is: \", len(matching_files))\n",
    "\n",
    "    # --- End of new matching logic ---\n",
    "\n",
    "    # Construct output path\n",
    "    base, ext = os.path.splitext(csv_path)\n",
    "    filtered_csv_path = f\"{base}{suffix}{ext}\"\n",
    "\n",
    "    # Save filtered DataFrame\n",
    "    filtered_df.to_csv(filtered_csv_path, sep=\";\", index=False)\n",
    "    print(f\"Filtered metadata saved to: {filtered_csv_path}\")\n",
    "    print(f\"Filtered rows: {len(filtered_df)}\")\n",
    "\n",
    "    return filtered_csv_path, matching_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "03513a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to parse filename: 201125_PIIOVo4ProinflamLP_4T1R_Bld_D7_C6_002.fcs\n",
      "WARNING:root:Failed to parse filename: 201125_PIIOVo4ProinflamLP_4T1R_Bld_D7_C6eC10000_003.fcs\n",
      "WARNING:root:Failed to parse filename: 200908_piIOVo2_MacMicLP_4T1R_Bld_D14_MacMicPlasmaStd.fcs\n",
      "WARNING:root:Failed to parse filename: 201125_PIIOVo4ProinflamLP_4T1R_Bld_D7_C6eC10000_001.fcs\n",
      "WARNING:root:Failed to parse filename: 200908_piIOVo2_ProinflamLP_4T1R_Bld_D7_ProinflPlasmaStd.fcs\n",
      "WARNING:root:Failed to parse filename: 201125_PIIOVo4ProinflamLP_4T1R_Bld_D7_C7_006.fcs\n",
      "WARNING:root:Failed to parse filename: 201125_PIIOVo4ProinflamLP_4T1R_Bld_D7_C6_004.fcs\n",
      "WARNING:root:Failed to parse filename: 200908_piIOVo2_ProinflamLP_4T1R_Bld_D14_ProinflPlasmaStd.fcs\n",
      "WARNING:root:Failed to parse filename: 201125_PIIOVo4ProinflamLP_4T1R_Bld_D7_C6eC10000_005.fcs\n",
      "WARNING:root:Failed to parse filename: 201125_PIIOVo4ProinflamLP_4T1R_Bld_D7_C7_010.fcs\n",
      "WARNING:root:Failed to parse filename: 200908_piIOVo2_MacMicLP_4T1R_Bld_D7_MacMicPlasmaStd.fcs\n",
      "WARNING:root:Failed to parse filename: 201125_PIIOVo4ProinflamLP_4T1R_Bld_D7_C7eC10000_009.fcs\n",
      "WARNING:root:Failed to parse filename: 201125_PIIOVo4ProinflamLP_4T1R_Bld_D7_C7eC10000_007.fcs\n",
      "WARNING:root:Failed to parse filename: 201125_PIIOVo4ProinflamLP_4T1R_Bld_D7_C7_008.fcs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching files len is:  341\n",
      "Filtered metadata saved to: Y:/g/data/eu59/data_flowmop/fig_4_data/ANUDC_16/anu_dc_metadata_plasmafiltered.csv\n",
      "Filtered rows: 204\n"
     ]
    }
   ],
   "source": [
    "_, filtered_plasma_files = filter_and_save_metadata_plasma(\n",
    "    csv_path = csv_path,\n",
    "    dir_files=os.listdir(raw_plasma_location),\n",
    "    columns_to_keep=COLUMNS_TO_KEEP,\n",
    "    suffix=\"_plasmafiltered\"\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "290431fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 244 files to Y:/g/data/eu59/data_flowmop/fig_4_data/ANUDC_16\\FCS_files\\cells_panel_filtered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "def copy_files_by_filename(\n",
    "    source_dir: str,\n",
    "    filenames: List[str],\n",
    "    dest_dir: str,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Copies files from the source directory to the destination directory if their full filenames match any in the provided list.\n",
    "\n",
    "    Args:\n",
    "        source_dir (str): Path to the directory containing the source files.\n",
    "        filenames (List[str]): List of full filenames (including extension) to match and copy.\n",
    "        dest_dir (str): Path to the directory where matched files will be copied.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of file paths that were successfully copied.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the source directory does not exist.\n",
    "\n",
    "    Example:\n",
    "        >>> copy_files_by_filename(\n",
    "        ...     source_dir=\"raw_data\",\n",
    "        ...     filenames=[\"200825_PIIOVo2_FC_4T1R_Bld_D7_C1_003.fcs\", \"20222_MPTLVo4_FC_4T1R_Bld_D14_C5_022.fcs\"],\n",
    "        ...     dest_dir=\"filtered_data\"\n",
    "        ... )\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(source_dir):\n",
    "        raise FileNotFoundError(f\"Source directory not found: {source_dir}\")\n",
    "\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "    copied_files: List[str] = []\n",
    "\n",
    "    # Use a set for O(1) lookup\n",
    "    filename_set = set(filenames)\n",
    "\n",
    "    for fname in os.listdir(source_dir):\n",
    "        if fname in filename_set:\n",
    "            src_path = os.path.join(source_dir, fname)\n",
    "            dst_path = os.path.join(dest_dir, fname)\n",
    "            try:\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "                copied_files.append(dst_path)\n",
    "            except OSError as e:\n",
    "                logging.warning(f\"Error copying {src_path} to {dst_path}: {e}\")\n",
    "\n",
    "    print(f\"Copied {len(copied_files)} files to {dest_dir}\")\n",
    "    return copied_files\n",
    "\n",
    "# Usage example:\n",
    "# Suppose you have a list of full filenames you want to copy:\n",
    "# filtered_filenames = filtered_cell_files\n",
    "copied = copy_files_by_filename(\n",
    "    source_dir=raw_cells_location,\n",
    "    filenames=filtered_cell_files,\n",
    "    dest_dir=os.path.join(root, \"FCS_files\", \"cells_panel_filtered\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "227540ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 341 files to Y:/g/data/eu59/data_flowmop/fig_4_data/ANUDC_16\\FCS_files\\plasma_panel_filtered\n"
     ]
    }
   ],
   "source": [
    "copied = copy_files_by_filename(\n",
    "    source_dir=raw_plasma_location,\n",
    "    filenames=filtered_plasma_files,\n",
    "    dest_dir=os.path.join(root, \"FCS_files\", \"plasma_panel_filtered\")\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
