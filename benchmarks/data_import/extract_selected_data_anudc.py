import pathlib
import random
import shutil
from typing import List, Dict, Any, Set, Tuple, Optional

import pandas as pd
import flowio
from sklearn.model_selection import train_test_split

# --- Configuration Constants ---
# IMPORTANT: User should verify these paths and names
# Path to the CSV file generated by clean_anudc.py
INPUT_FILTERED_METADATA_CSV: pathlib.Path = pathlib.Path(
    "/g/data/eu59/data_flowmop/ANUDC_16/filtered_anu_dc_metadata.csv"
)
# Base directory where FCS files are stored
FCS_FILES_BASE_DIR: pathlib.Path = pathlib.Path(
    "/g/data/eu59/data_flowmop/ANUDC_16/FCS_files/cells_panel"
)
# Output path for the CSV with selected and processed columns
OUTPUT_SELECTED_COLUMNS_CSV: pathlib.Path = pathlib.Path(
    "/g/data/eu59/data_flowmop/ANUDC_16/selected_metadata_columns_cellspanel.csv"
)
# Output path for the CSV with marker names from sampled FCS files
OUTPUT_MARKER_NAMES_CSV: pathlib.Path = pathlib.Path(
    "/g/data/eu59/data_flowmop/ANUDC_16/sampled_marker_names_cellspanel.csv"
)
# Output path for the filtered dataset with existing files only
OUTPUT_FILTERED_LABEL_CSV: pathlib.Path = pathlib.Path(
    "/g/data/eu59/data_flowmop/ANUDC_16/filtered_label_dataset_cellspanel.csv"
)
# Output paths for train/test splits
OUTPUT_TRAIN_CSV: pathlib.Path = pathlib.Path(
    "/g/data/eu59/data_flowmop/ANUDC_16/train_cellspanel.csv"
)
OUTPUT_TEST_CSV: pathlib.Path = pathlib.Path(
    "/g/data/eu59/data_flowmop/ANUDC_16/test_cellspanel.csv"
)
# Base directory for train/test file organization
TRAIN_TEST_BASE_DIR: pathlib.Path = pathlib.Path(
    "/g/data/eu59/data_flowmop/ANUDC_16/train_test_split_cellspanel"
)

# Column names
FILENAME_COLUMN_IN_METADATA: str = "filename"
USER_SPECIFIED_COLUMN_TO_EXTRACT: str = "day" # The column to process and extract
PROCESSED_USER_COLUMN_NAME: str = f"{USER_SPECIFIED_COLUMN_TO_EXTRACT}_numeric" # Name for the cleaned column

NUMBER_OF_FCS_FILES_TO_SAMPLE: int = 10


def filter_existing_files(
    metadata_df: pd.DataFrame,
    fcs_base_dir: pathlib.Path,
    filename_col: str,
    output_filtered_csv: pathlib.Path,
) -> pd.DataFrame:
    """
    Filter metadata to only include rows where the corresponding FCS file exists.
    
    Args:
        metadata_df: DataFrame containing metadata with filenames
        fcs_base_dir: Base directory where FCS files are located
        filename_col: Name of the column containing filenames
        output_filtered_csv: Path to save the filtered dataset
        
    Returns:
        Filtered DataFrame containing only rows with existing files
    """
    print(f"Checking file existence in {fcs_base_dir}")
    
    existing_files = []
    missing_files_details = [] # Changed to store more details
    
    for idx, filename in enumerate(metadata_df[filename_col]):
        original_filename_in_metadata = filename # Keep original for logging
        fcs_file_path = fcs_base_dir / filename
        
        # Try different file path variations
        file_exists = False
        paths_tried = [] # Log paths attempted

        # Attempt 1: Direct path from metadata
        paths_tried.append(str(fcs_file_path))
        if fcs_file_path.is_file():
            file_exists = True
        else:
            # Attempt 2: Add .fcs suffix if not already there implicitly or explicitly
            # (Handles cases where metadata filename might be 'file' and actual is 'file.fcs')
            # (Also handles if metadata is 'file.FCS' but actual is 'file.FCS.fcs' - less likely but good to check)
            current_suffix_lower = fcs_file_path.suffix.lower()
            if current_suffix_lower != '.fcs':
                fcs_path_with_dot_fcs = fcs_file_path.with_suffix(fcs_file_path.suffix + '.fcs')
                paths_tried.append(str(fcs_path_with_dot_fcs))
                if fcs_path_with_dot_fcs.is_file():
                    file_exists = True
                    fcs_file_path = fcs_path_with_dot_fcs # Update to the found path

            # Attempt 3: If original filename in metadata doesn't end with .fcs (any case)
            # and previous attempts failed, try appending .fcs to the original filename string.
            if not file_exists and not original_filename_in_metadata.lower().endswith('.fcs'):
                fcs_file_path_with_suffix_added_to_original = fcs_base_dir / (original_filename_in_metadata + ".fcs")
                paths_tried.append(str(fcs_file_path_with_suffix_added_to_original))
                if fcs_file_path_with_suffix_added_to_original.is_file():
                    file_exists = True
                    fcs_file_path = fcs_file_path_with_suffix_added_to_original # Update
        
        if file_exists:
            existing_files.append(idx)
        else:
            missing_files_details.append({
                "metadata_filename": original_filename_in_metadata,
                "paths_tried": paths_tried
            })
    
    print(f"Found {len(existing_files)} existing files out of {len(metadata_df)} total")
    if missing_files_details:
        print(f"Could not find {len(missing_files_details)} files. Details for the first 10 missing files:")
        for i, detail in enumerate(missing_files_details[:10]):
            print(f"  {i+1}. Metadata filename: '{detail['metadata_filename']}', Paths tried: {detail['paths_tried']}")
    
    # Filter DataFrame to only include existing files
    filtered_df = metadata_df.iloc[existing_files].copy()
    
    # Save filtered dataset
    try:
        output_filtered_csv.parent.mkdir(parents=True, exist_ok=True)
        filtered_df.to_csv(output_filtered_csv, index=False)
        print(f"Filtered dataset saved to: {output_filtered_csv}")
    except Exception as e:
        print(f"Error saving filtered dataset to {output_filtered_csv}: {e}")
        raise
    
    return filtered_df


def create_stratified_split_and_move_files(
    filtered_df: pd.DataFrame,
    fcs_base_dir: pathlib.Path,
    filename_col: str,
    label_col: str,
    train_csv_path: pathlib.Path,
    test_csv_path: pathlib.Path,
    train_test_base_dir: pathlib.Path,
    test_size: float = 0.2,
    random_state: int = 42,
) -> None:
    """
    Create stratified train/test split and move files to respective directories.
    
    Args:
        filtered_df: DataFrame with existing files only
        fcs_base_dir: Source directory containing FCS files
        filename_col: Column name containing filenames
        label_col: Column name containing binary labels
        train_csv_path: Path to save train CSV
        test_csv_path: Path to save test CSV
        train_test_base_dir: Base directory for train/test folders
        test_size: Proportion of data for test set
        random_state: Random seed for reproducibility
    """
    print(f"Creating stratified split with test_size={test_size}")
    
    # Perform stratified split
    train_df, test_df = train_test_split(
        filtered_df,
        test_size=test_size,
        stratify=filtered_df[label_col],
        random_state=random_state
    )
    
    # Save train and test CSVs
    try:
        train_csv_path.parent.mkdir(parents=True, exist_ok=True)
        test_csv_path.parent.mkdir(parents=True, exist_ok=True)
        
        train_df.to_csv(train_csv_path, index=False)
        test_df.to_csv(test_csv_path, index=False)
        
        print(f"Train CSV saved to: {train_csv_path}")
        print(f"Test CSV saved to: {test_csv_path}")
    except Exception as e:
        print(f"Error saving train/test CSVs: {e}")
        raise
    
    # Create train and test directories
    train_dir = train_test_base_dir / "train"
    test_dir = train_test_base_dir / "test"
    
    train_dir.mkdir(parents=True, exist_ok=True)
    test_dir.mkdir(parents=True, exist_ok=True)
    
    # Move files to train directory
    copy_files_to_directory(train_df[filename_col], fcs_base_dir, train_dir, "train")
    
    # Move files to test directory
    copy_files_to_directory(test_df[filename_col], fcs_base_dir, test_dir, "test")
    
    print(f"Train set: {len(train_df)} files")
    print(f"Test set: {len(test_df)} files")
    print(f"Class distribution in train: {train_df[label_col].value_counts().to_dict()}")
    print(f"Class distribution in test: {test_df[label_col].value_counts().to_dict()}")

def copy_files_to_directory(
    filenames: pd.Series,
    source_dir: pathlib.Path,
    dest_dir: pathlib.Path,
    split_name: str,
) -> None:
    """
    Move files from source directory to destination directory.
    
    Args:
        filenames: Series of filenames to move
        source_dir: Source directory containing files
        dest_dir: Destination directory
        split_name: Name of the split (for logging)
    """
    moved_count = 0
    missing_count = 0
    
    for filename in filenames:
        source_file = source_dir / filename
        
        # Try different file path variations (same logic as filter_existing_files)
        if not source_file.is_file():
            if source_file.with_suffix(source_file.suffix + '.fcs').is_file():
                source_file = source_file.with_suffix(source_file.suffix + '.fcs')
            elif not source_file.name.lower().endswith('.fcs'):
                source_file_with_suffix = source_dir / (filename + ".fcs")
                if source_file_with_suffix.is_file():
                    source_file = source_file_with_suffix
        
        if source_file.is_file():
            dest_file = dest_dir / source_file.name
            try:
                shutil.copy2(str(source_file), str(dest_file))
                moved_count += 1
            except Exception as e:
                print(f"Error moving {source_file} to {dest_file}: {e}")
        else:
            print(f"Warning: File not found for moving: {source_file}")
            missing_count += 1
    
    print(f"Moved {moved_count} files to {split_name} directory")
    if missing_count > 0:
        print(f"Could not move {missing_count} files (not found)")


def extract_and_save_selected_columns(
    input_csv_path: pathlib.Path,
    output_csv_path: pathlib.Path,
    filename_col: str,
    user_col_to_extract: str,
    processed_user_col_name: str,
) -> pd.DataFrame:
    """
    Reads a metadata CSV, processes a user-specified column, and saves selected columns.

    The user-specified column (e.g., 'day') will have non-numeric characters
    removed and then be converted to an integer type.

    Args:
        input_csv_path: Path to the input metadata CSV file.
        output_csv_path: Path where the filtered metadata CSV will be saved.
        filename_col: Name of the column containing filenames.
        user_col_to_extract: Name of the user-specified column to process.
        processed_user_col_name: The name for the new column after processing.

    Returns:
        A pandas DataFrame containing all original columns, plus the processed
        user column. This DataFrame can be used for further operations like sampling.

    Raises:
        FileNotFoundError: If the input CSV file does not exist.
        KeyError: If required columns are not found in the CSV.
        ValueError: If processing the user-specified column results in values
                    that cannot be converted to integers (e.g., empty strings
                    after stripping non-digits).
    """
    if not input_csv_path.is_file():
        raise FileNotFoundError(f"Input CSV not found: {input_csv_path}")

    try:
        metadata_df = pd.read_csv(input_csv_path)
    except Exception as e:
        print(f"Error reading metadata CSV {input_csv_path}: {e}")
        raise

    if filename_col not in metadata_df.columns:
        msg = f"Filename column '{filename_col}' not found in {input_csv_path}."
        raise KeyError(msg)
    if user_col_to_extract not in metadata_df.columns:
        msg = (
            f"User-specified column '{user_col_to_extract}' not found "
            f"in {input_csv_path}."
        )
        raise KeyError(msg)

    # Process the user-specified column
    # Convert to string, remove non-digits, then attempt to convert to numeric
    cleaned_series = (
        metadata_df[user_col_to_extract]
        .astype(str)
        .str.replace(r"\D+", "", regex=True)  # Removed extra backslash
    )

    # Replace empty strings (which result if the original had no digits) with NaN
    # so pd.to_numeric can handle them
    cleaned_series_with_na = cleaned_series.replace("", pd.NA)
    numeric_series = pd.to_numeric(cleaned_series_with_na, errors="coerce")

    # Check for conversion errors (NaNs)
    if numeric_series.isnull().any():
        problematic_indices = numeric_series[numeric_series.isnull()].index
        problematic_original_values = metadata_df[user_col_to_extract][
            problematic_indices
        ].tolist()
        msg = (
            f"Error converting column '{user_col_to_extract}' to numeric. "
            f"Problematic original values: {problematic_original_values}. "
            "These values resulted in non-numeric entries after stripping non-digits."
        )
        raise ValueError(msg)

    metadata_df[processed_user_col_name] = numeric_series.astype(int)

    # Select and save the specified columns
    output_df = metadata_df[[filename_col, processed_user_col_name]].copy()

    try:
        output_csv_path.parent.mkdir(parents=True, exist_ok=True)
        output_df.to_csv(output_csv_path, index=False)
        print(f"Selected columns saved to: {output_csv_path}")
    except Exception as e:
        print(f"Error saving selected columns CSV to {output_csv_path}: {e}")
        raise
    
    # Return the full dataframe with the new processed column for sampling
    return metadata_df

def main() -> None:
    """
    Main function to orchestrate the data extraction and processing.
    """
    print("Starting data extraction process...")
    try:
        # Ensure output directories exist (though functions also do this)
        INPUT_FILTERED_METADATA_CSV.parent.mkdir(parents=True, exist_ok=True)
        OUTPUT_SELECTED_COLUMNS_CSV.parent.mkdir(parents=True, exist_ok=True)
        OUTPUT_MARKER_NAMES_CSV.parent.mkdir(parents=True, exist_ok=True)
        OUTPUT_FILTERED_LABEL_CSV.parent.mkdir(parents=True, exist_ok=True)
        OUTPUT_TRAIN_CSV.parent.mkdir(parents=True, exist_ok=True)
        OUTPUT_TEST_CSV.parent.mkdir(parents=True, exist_ok=True)
        TRAIN_TEST_BASE_DIR.mkdir(parents=True, exist_ok=True)
        FCS_FILES_BASE_DIR.mkdir(parents=True, exist_ok=True) # Ensure base FCS dir for script to run if empty

        print(f"Step 1: Processing metadata from {INPUT_FILTERED_METADATA_CSV}")
        full_metadata_df = extract_and_save_selected_columns(
            input_csv_path=INPUT_FILTERED_METADATA_CSV,
            output_csv_path=OUTPUT_SELECTED_COLUMNS_CSV,
            filename_col=FILENAME_COLUMN_IN_METADATA,
            user_col_to_extract=USER_SPECIFIED_COLUMN_TO_EXTRACT,
            processed_user_col_name=PROCESSED_USER_COLUMN_NAME,
        )
        
        print(f"\\nStep 2: Filtering dataset to only include existing FCS files")
        filtered_df = filter_existing_files(
            metadata_df=full_metadata_df,
            fcs_base_dir=FCS_FILES_BASE_DIR,
            filename_col=FILENAME_COLUMN_IN_METADATA,
            output_filtered_csv=OUTPUT_FILTERED_LABEL_CSV,
        )
        
        print(f"\\nStep 3: Creating stratified train/test split and moving files")
        create_stratified_split_and_move_files(
            filtered_df=filtered_df,
            fcs_base_dir=FCS_FILES_BASE_DIR,
            filename_col=FILENAME_COLUMN_IN_METADATA,
            label_col=PROCESSED_USER_COLUMN_NAME,
            train_csv_path=OUTPUT_TRAIN_CSV,
            test_csv_path=OUTPUT_TEST_CSV,
            train_test_base_dir=TRAIN_TEST_BASE_DIR,
        )
        
        print("\\nProcess completed successfully.")

    except FileNotFoundError as e:
        print(f"Process failed: A required file or directory was not found. Details: {e}")
    except KeyError as e:
        print(f"Process failed: A required column was not found. Details: {e}")
    except ValueError as e:
        print(f"Process failed: Data validation error. Details: {e}")
    except Exception as e:
        print(f"An unexpected error occurred during the process: {e}")


if __name__ == "__main__":
    main() 